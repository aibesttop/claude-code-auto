# V3 ç”Ÿäº§çº§å®Œå–„è®¡åˆ’ (Production-Ready V3 Roadmap)

> åŸºäºç°æœ‰ä»£ç åˆ†æå’Œ 2024/2025 æœ€ä½³å®è·µçš„å¯æ‰§è¡Œå®Œå–„è®¡åˆ’
>
> ç‰ˆæœ¬: V3.1
> åˆ›å»ºæ—¶é—´: 2025-11-21
> ç›®æ ‡: å°† V3 ä»åŸå‹æå‡åˆ°ç”Ÿäº§çº§è´¨é‡

---

## ğŸ“‹ ç°çŠ¶è¯„ä¼° (Current State Assessment)

### âœ… å·²å®ç°çš„æ ¸å¿ƒèƒ½åŠ›

| æ¨¡å— | å®ç°çŠ¶æ€ | æ–‡ä»¶ä½ç½® | è´¨é‡è¯„çº§ |
|------|---------|----------|---------|
| Persona Engine | âœ… åŸºç¡€ç‰ˆ | `core/agents/persona.py` | â­â­â­ (å¯ç”¨) |
| Researcher Agent | âœ… å¸¦ç¼“å­˜ | `core/agents/researcher.py` | â­â­â­â­ (è‰¯å¥½) |
| Event Store | âœ… å®Œæ•´ | `core/events.py` | â­â­â­â­ (è‰¯å¥½) |
| Cost Tracker | âœ… å®Œæ•´ | `core/events.py` | â­â­â­â­ (è‰¯å¥½) |
| State Manager | âœ… å®Œæ•´ | `state_manager.py` | â­â­â­â­ (è‰¯å¥½) |
| Tool Registry | âœ… åŸºç¡€ç‰ˆ | `core/tool_registry.py` | â­â­â­ (å¯ç”¨) |
| Config System | âœ… Pydantic | `config.py` | â­â­â­â­â­ (ä¼˜ç§€) |

### âŒ ç¼ºå¤±çš„å…³é”®èƒ½åŠ›

| èƒ½åŠ› | ä¼˜å…ˆçº§ | é£é™©ç­‰çº§ | é¢„è®¡å·¥ä½œé‡ |
|-----|-------|---------|-----------|
| **Persona ä¼˜åŒ–ä¸å‹ç¼©** | ğŸ”´ P0 | ğŸŸ¡ ä¸­ | 2-3 å¤© |
| **é¢„ç®—ç®¡ç†ç³»ç»Ÿ** | ğŸ”´ P0 | ğŸŸ  é«˜ | 2-3 å¤© |
| **OpenTelemetry åˆ†å¸ƒå¼è¿½è¸ª** | ğŸŸ  P1 | ğŸŸ¢ ä½ | 3-4 å¤© |
| **çŠ¶æ€ Checkpoint/Rollback** | ğŸŸ  P1 | ğŸŸ¡ ä¸­ | 2-3 å¤© |
| **Researcher NLI éªŒè¯** | ğŸŸ  P1 | ğŸŸ¡ ä¸­ | 3-4 å¤© |
| **Tool Composer (å®‰å…¨ç‰ˆ)** | ğŸŸ  P1 | ğŸŸ  é«˜ | 4-5 å¤© |
| **Sub-Agent Orchestrator** | ğŸŸ¡ P2 | ğŸŸ  é«˜ | 5-7 å¤© |
| **æ²™ç®±æ‰§è¡Œç³»ç»Ÿ** | ğŸŸ¡ P2 | ğŸ”´ æé«˜ | 5-7 å¤© |
| **å†²çªæ£€æµ‹ä¸è§£å†³** | ğŸŸ¡ P2 | ğŸŸ¡ ä¸­ | 3-4 å¤© |

---

## ğŸ¯ å¼€å‘è·¯çº¿å›¾ (Development Roadmap)

### Phase 0: åŸºç¡€è®¾æ–½å¼ºåŒ– (Foundation) - 1 å‘¨

**ç›®æ ‡**: ä¸ºç”Ÿäº§çº§è¿è¡Œæ‰“ä¸‹åšå®åŸºç¡€

#### Task 0.1: é¢„ç®—ç®¡ç†ç³»ç»Ÿ (2 å¤©)
**å®ç°æ–‡ä»¶**: `core/budget_manager.py`

```python
class BudgetManager:
    """æ™ºèƒ½é¢„ç®—ç®¡ç†å™¨ï¼Œæ”¯æŒå¤šç²’åº¦æˆæœ¬æ§åˆ¶"""

    def __init__(self, daily_budget: float = 100.0):
        self.daily_budget = daily_budget
        self.budgets = {
            "daily": daily_budget,
            "per_iteration": daily_budget / 10,  # å•æ¬¡è¿­ä»£é¢„ç®—
            "researcher": daily_budget * 0.3,     # ç ”ç©¶å æ¯”30%
            "executor": daily_budget * 0.6,       # æ‰§è¡Œå æ¯”60%
        }
        self.cost_tracker = None  # æ³¨å…¥ CostTracker

    async def check_budget(self, operation: str, estimated_cost: float) -> Dict:
        """é¢„ç®—æ£€æŸ¥ + é™çº§ç­–ç•¥"""
        if not self._has_budget(operation, estimated_cost):
            return await self._apply_fallback(operation)
        return {"allowed": True, "strategy": "primary"}

    def _apply_fallback(self, operation: str) -> Dict:
        """é™çº§ç­–ç•¥"""
        if operation == "web_search":
            return {"allowed": True, "strategy": "cache_only"}
        elif operation == "llm_call":
            return {"allowed": True, "strategy": "smaller_model"}
        return {"allowed": False, "strategy": "blocked"}
```

**é›†æˆç‚¹**:
- åœ¨ `main_v3.py` åˆå§‹åŒ–æ—¶åˆ›å»º `BudgetManager`
- åœ¨æ¯æ¬¡ LLM è°ƒç”¨å‰æ£€æŸ¥é¢„ç®—
- åœ¨ `ResearcherAgent.research()` ä¸­é›†æˆ
- æ·»åŠ å®æ—¶é¢„ç®—ç›‘æ§åˆ° Web UI

**éªŒæ”¶æ ‡å‡†**:
- âœ… å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%
- âœ… æˆæœ¬è¶…æ ‡æ—¶è‡ªåŠ¨é™çº§åˆ° Haiku
- âœ… ç ”ç©¶æŸ¥è¯¢è¶…é¢„ç®—æ—¶ä½¿ç”¨ç¼“å­˜
- âœ… å®æ—¶é¢„ç®—ä»ªè¡¨ç›˜å¯è§†åŒ–

---

#### Task 0.2: OpenTelemetry åˆ†å¸ƒå¼è¿½è¸ª (3 å¤©)
**å®ç°æ–‡ä»¶**: `core/observability.py`

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.jaeger import JaegerExporter

class ObservabilityLayer:
    """ç»Ÿä¸€çš„å¯è§‚æµ‹æ€§å±‚"""

    def __init__(self, service_name: str = "claude-code-auto"):
        # åˆå§‹åŒ– OpenTelemetry
        trace.set_tracer_provider(TracerProvider())
        self.tracer = trace.get_tracer(__name__)

        # Jaeger exporter (å¯é€‰ï¼Œå¼€å‘ç¯å¢ƒä½¿ç”¨)
        if os.getenv("JAEGER_ENABLED", "false") == "true":
            jaeger_exporter = JaegerExporter(
                agent_host_name="localhost",
                agent_port=6831,
            )
            trace.get_tracer_provider().add_span_processor(
                BatchSpanProcessor(jaeger_exporter)
            )

    def trace_agent(self, agent_name: str):
        """Agentæ‰§è¡Œè¿½è¸ªè£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                with self.tracer.start_as_current_span(
                    f"agent.{agent_name}",
                    attributes={
                        "agent.name": agent_name,
                        "agent.type": type(args[0]).__name__
                    }
                ) as span:
                    try:
                        result = await func(*args, **kwargs)
                        span.set_attribute("status", "success")
                        return result
                    except Exception as e:
                        span.record_exception(e)
                        span.set_attribute("status", "error")
                        raise
            return wrapper
        return decorator
```

**é›†æˆç‚¹**:
- è£…é¥°æ‰€æœ‰ Agent çš„ä¸»è¦æ–¹æ³• (`planner.get_next_step`, `executor.execute_task`, `researcher.research`)
- åœ¨ `main_v3.py` ä¸­åˆå§‹åŒ–
- å¯¼å‡ºåˆ° Jaeger UI (å¼€å‘) æˆ– JSON (ç”Ÿäº§)

**å¯é€‰ä¾èµ–**:
```bash
pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-jaeger
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ‰€æœ‰ Agent è°ƒç”¨é“¾è·¯å¯è§†åŒ–
- âœ… å¼‚å¸¸è‡ªåŠ¨æ•è·åˆ° span
- âœ… ç«¯åˆ°ç«¯å»¶è¿Ÿå¯è¿½è¸ª

---

#### Task 0.3: çŠ¶æ€ Checkpoint ä¸ Rollback (2 å¤©)
**å¢å¼ºæ–‡ä»¶**: `state_manager.py`

åœ¨ç°æœ‰ `StateManager` ä¸­æ·»åŠ :

```python
class StateManager:
    # ... ç°æœ‰ä»£ç  ...

    def __init__(self, state_file_path: Path, max_checkpoints: int = 10):
        self.state_file_path = state_file_path
        self.checkpoint_dir = state_file_path.parent / "checkpoints"
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.max_checkpoints = max_checkpoints
        self._state: Optional[ExecutionState] = None

    def create_checkpoint(self, label: str = None) -> Path:
        """åˆ›å»ºçŠ¶æ€å¿«ç…§"""
        if self._state is None:
            raise RuntimeError("æ²¡æœ‰å¯ä¿å­˜çš„çŠ¶æ€")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        label_suffix = f"_{label}" if label else ""
        checkpoint_name = f"checkpoint_{timestamp}{label_suffix}.json"
        checkpoint_path = self.checkpoint_dir / checkpoint_name

        # ä¿å­˜å¿«ç…§
        with open(checkpoint_path, 'w', encoding='utf-8') as f:
            json.dump(self._state.to_dict(), f, indent=2, ensure_ascii=False)

        # æ¸…ç†æ—§å¿«ç…§
        self._cleanup_old_checkpoints()

        return checkpoint_path

    def rollback_to_checkpoint(self, checkpoint_path: Path) -> ExecutionState:
        """å›æ»šåˆ°æŒ‡å®šå¿«ç…§"""
        self._state = ExecutionState.load(checkpoint_path)
        self.save()  # æ›´æ–°ä¸»çŠ¶æ€æ–‡ä»¶
        return self._state

    def list_checkpoints(self) -> List[Path]:
        """åˆ—å‡ºæ‰€æœ‰å¿«ç…§"""
        return sorted(self.checkpoint_dir.glob("checkpoint_*.json"), reverse=True)

    def _cleanup_old_checkpoints(self):
        """ä¿ç•™æœ€æ–° N ä¸ªå¿«ç…§"""
        checkpoints = self.list_checkpoints()
        if len(checkpoints) > self.max_checkpoints:
            for old_checkpoint in checkpoints[self.max_checkpoints:]:
                old_checkpoint.unlink()
```

**é›†æˆç‚¹**:
- åœ¨æ¯æ¬¡æˆåŠŸçš„è¿­ä»£ååˆ›å»º checkpoint
- åœ¨é‡åˆ°é”™è¯¯æ—¶è‡ªåŠ¨å›æ»šåˆ°æœ€åä¸€ä¸ªæˆåŠŸçš„ checkpoint
- æ·»åŠ  CLI å‘½ä»¤æ‰‹åŠ¨å›æ»š

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ¯æ¬¡è¿­ä»£åè‡ªåŠ¨åˆ›å»ºå¿«ç…§
- âœ… é”™è¯¯æ—¶å¯å›æ»šåˆ°ä¹‹å‰çš„çŠ¶æ€
- âœ… ä¿ç•™æœ€è¿‘ 10 ä¸ªå¿«ç…§

---

### Phase 1: æ ¸å¿ƒèƒ½åŠ›å¢å¼º (Core Enhancement) - 2 å‘¨

#### Task 1.1: Persona ä¼˜åŒ–ä¸å‹ç¼© (3 å¤©)
**å¢å¼ºæ–‡ä»¶**: `core/agents/persona.py`

æ·»åŠ åŠ¨æ€ Persona ä¼˜åŒ–:

```python
class PersonaEngine:
    # ... ç°æœ‰ä»£ç  ...

    def __init__(self, persona_config: dict = None, enable_optimization: bool = True):
        self.current_persona = PERSONAS["default"]
        self.switch_history: List[PersonaSwitch] = []
        self.enable_optimization = enable_optimization
        self.context_window = 8000  # Personaæœ€å¤§tokenæ•°

        if persona_config:
            self._load_config(persona_config)

    async def build_optimized_persona(
        self,
        role: str,
        task_context: dict,
        llm_client = None
    ) -> str:
        """åŠ¨æ€æ„å»ºå¹¶ä¼˜åŒ– Persona"""
        base_persona = PERSONAS.get(role, PERSONAS["default"])
        full_prompt = base_persona.system_prompt

        # å¦‚æœç¦ç”¨ä¼˜åŒ–æˆ–æ²¡æœ‰LLMå®¢æˆ·ç«¯ï¼Œç›´æ¥è¿”å›
        if not self.enable_optimization or not llm_client:
            return full_prompt

        # ä½¿ç”¨ LLM å‹ç¼© Personaï¼ˆå…ƒæç¤ºï¼‰
        optimization_prompt = f"""
Given the task: {task_context.get('goal', 'N/A')}

Compress the following persona to ~{self.context_window} tokens, keeping only the most relevant parts for the task:

{full_prompt}

Return ONLY the compressed persona, no explanations.
"""

        try:
            optimized, _ = await llm_client(
                optimization_prompt,
                model="claude-3-haiku-20240307",  # ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹ä¼˜åŒ–
                timeout=30
            )

            # éªŒè¯é•¿åº¦ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
            if len(optimized) < len(full_prompt) * 0.9:  # è‡³å°‘å‹ç¼©10%
                return optimized.strip()
        except Exception as e:
            logger.warning(f"Personaä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç‰ˆæœ¬: {e}")

        return full_prompt
```

**é›†æˆç‚¹**:
- åœ¨ `ExecutorAgent` ä¸­è°ƒç”¨ä¼˜åŒ–åçš„ Persona
- ç¼“å­˜ä¼˜åŒ–åçš„ Persona (æŒ‰ role + task hash)

**éªŒæ”¶æ ‡å‡†**:
- âœ… Persona token ä½¿ç”¨å‡å°‘ > 20%
- âœ… ä»»åŠ¡ç›¸å…³æ€§æå‡ï¼ˆäººå·¥è¯„ä¼°ï¼‰

---

#### Task 1.2: Researcher NLI éªŒè¯ä¸é‡æ’åº (4 å¤©)
**å¢å¼ºæ–‡ä»¶**: `core/agents/researcher.py`

```python
from sentence_transformers import CrossEncoder  # é‡æ’åºæ¨¡å‹

class ResearcherAgent:
    # ... ç°æœ‰ä»£ç  ...

    def __init__(self, *args, enable_reranking: bool = True, enable_nli: bool = True, **kwargs):
        # ... ç°æœ‰åˆå§‹åŒ– ...
        self.enable_reranking = enable_reranking
        self.enable_nli = enable_nli

        # åŠ è½½é‡æ’åºæ¨¡å‹ï¼ˆè½»é‡çº§ï¼‰
        if enable_reranking:
            self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

        # NLI æ¨¡å‹ï¼ˆç”¨äºå¹»è§‰æ£€æµ‹ï¼‰
        if enable_nli:
            from transformers import pipeline
            self.nli_pipeline = pipeline(
                "text-classification",
                model="microsoft/deberta-v3-base-tasksource-nli"
            )

    async def research(self, query: str, use_cache: bool = True) -> Dict:
        """å¢å¼ºç‰ˆç ”ç©¶æµç¨‹"""
        # ... ç°æœ‰ç¼“å­˜æ£€æŸ¥ ...

        # 1. æ‰§è¡Œæœç´¢
        search_result = web_search(query)

        # 2. é‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_reranking and isinstance(search_result, list):
            ranked_results = self._rerank_results(query, search_result)
        else:
            ranked_results = search_result

        # 3. LLM æ€»ç»“
        summary = await self._summarize_results(query, ranked_results)

        # 4. NLI éªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_nli:
            quality_score = self._verify_consistency(summary, ranked_results)

            if quality_score < 0.6:  # ä½è´¨é‡ï¼Œæ ‡è®°è­¦å‘Š
                logger.warning(f"ç ”ç©¶è´¨é‡è¯„åˆ†ä½: {quality_score:.2f}")
                summary = f"âš ï¸ ä½ç½®ä¿¡åº¦ç»“æœ (score={quality_score:.2f})\n\n{summary}"
        else:
            quality_score = None

        return {
            "summary": summary,
            "sources": ranked_results[:5] if isinstance(ranked_results, list) else [],
            "quality_score": quality_score
        }

    def _rerank_results(self, query: str, results: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨ CrossEncoder é‡æ’åº"""
        if not results:
            return results

        # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
        pairs = [[query, r.get("content", "")] for r in results]

        # æ‰“åˆ†
        scores = self.reranker.predict(pairs)

        # æ’åº
        ranked = sorted(
            zip(results, scores),
            key=lambda x: x[1],
            reverse=True
        )

        return [r for r, _ in ranked]

    def _verify_consistency(self, summary: str, sources: List[Dict]) -> float:
        """NLI éªŒè¯ï¼šæ£€æµ‹å¹»è§‰"""
        if not sources:
            return 0.5  # æ— æ³•éªŒè¯

        # æå–æºæ–‡æœ¬
        source_text = " ".join([s.get("content", "")[:500] for s in sources[:3]])

        # NLI æ¨ç†
        result = self.nli_pipeline(
            f"{source_text} [SEP] {summary}",
            truncation=True,
            max_length=512
        )

        # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
        # label: entailment (ä¸€è‡´), neutral, contradiction
        label_scores = {r['label']: r['score'] for r in result}
        consistency = label_scores.get('entailment', 0.0)

        return consistency
```

**æ–°å¢ä¾èµ–**:
```bash
pip install sentence-transformers transformers torch
```

**å¯é€‰ï¼šè½»é‡çº§éƒ¨ç½²æ–¹æ¡ˆ**
- ä½¿ç”¨ ONNX é‡åŒ–æ¨¡å‹å‡å°‘å†…å­˜å ç”¨
- æˆ–ä½¿ç”¨ Cohere Rerank API (ä»˜è´¹ä½†æ›´å¿«)

**éªŒæ”¶æ ‡å‡†**:
- âœ… æœç´¢ç»“æœç›¸å…³æ€§æå‡ > 30%
- âœ… å¹»è§‰æ£€æµ‹å‡†ç¡®ç‡ > 70%
- âœ… ä½è´¨é‡ç»“æœè‡ªåŠ¨æ ‡è®°

---

#### Task 1.3: Tool Composer (å®‰å…¨ç‰ˆ) (5 å¤©)
**æ–°å»ºæ–‡ä»¶**: `core/tool_composer.py`

**å…³é”®å†³ç­–**: ä¸å®ç°çœŸæ­£çš„ä»£ç ç”Ÿæˆï¼Œè€Œæ˜¯å®‰å…¨çš„å‡½æ•°ç»„åˆ

```python
class ToolComposer:
    """å®‰å…¨çš„å·¥å…·ç»„åˆå™¨ - ä¸æ‰§è¡Œä»»æ„ä»£ç """

    # é¢„å®šä¹‰çš„å®‰å…¨åŸè¯­
    SAFE_PRIMITIVES = {
        # HTTP æ“ä½œ
        "http_get": lambda url, **kw: requests.get(url, **kw).json(),
        "http_post": lambda url, data, **kw: requests.post(url, json=data, **kw).json(),

        # æ•°æ®è½¬æ¢
        "parse_json": json.loads,
        "parse_xml": lambda x: xmltodict.parse(x),
        "to_json": json.dumps,

        # åˆ—è¡¨æ“ä½œ
        "filter_list": lambda lst, condition: [
            x for x in lst if eval(condition, {"x": x, "__builtins__": {}})
        ],
        "map_list": lambda lst, transform: [
            eval(transform, {"x": x, "__builtins__": {}}) for x in lst
        ],
        "sort_list": lambda lst, key: sorted(lst, key=lambda x: x[key]),

        # å­—ç¬¦ä¸²æ“ä½œ
        "extract_regex": lambda text, pattern: re.findall(pattern, text),
        "replace_text": lambda text, old, new: text.replace(old, new),
        "split_text": lambda text, sep: text.split(sep),

        # æ•°å­¦æ“ä½œ
        "sum_values": sum,
        "avg_values": lambda lst: sum(lst) / len(lst) if lst else 0,
        "max_value": max,
        "min_value": min,
    }

    def compose_tool(self, spec: Dict) -> Callable:
        """
        æ ¹æ® JSON é…ç½®ç»„åˆå·¥å…·

        ç¤ºä¾‹é…ç½®:
        {
          "name": "fetch_github_stars",
          "description": "è·å– GitHub ä»“åº“æ˜Ÿæ•°",
          "steps": [
            {
              "primitive": "http_get",
              "args": {"url": "https://api.github.com/repos/{owner}/{repo}"}
            },
            {
              "primitive": "parse_json"
            },
            {
              "primitive": "extract_field",
              "args": {"field": "stargazers_count"}
            }
          ]
        }
        """
        name = spec.get("name", "custom_tool")
        steps = spec.get("steps", [])

        def composed_tool(*args, **kwargs):
            """ç»„åˆåçš„å·¥å…·å‡½æ•°"""
            result = args[0] if args else kwargs

            for step in steps:
                primitive_name = step.get("primitive")
                step_args = step.get("args", {})

                # è·å–åŸè¯­
                primitive = self.SAFE_PRIMITIVES.get(primitive_name)
                if not primitive:
                    raise ValueError(f"Unknown primitive: {primitive_name}")

                # æ¨¡æ¿æ›¿æ¢ï¼ˆæ”¯æŒ {var} è¯­æ³•ï¼‰
                resolved_args = self._resolve_templates(step_args, result, kwargs)

                # æ‰§è¡ŒåŸè¯­
                try:
                    if isinstance(result, dict):
                        result = primitive(**result, **resolved_args)
                    else:
                        result = primitive(result, **resolved_args)
                except Exception as e:
                    logger.error(f"å·¥å…·æ­¥éª¤å¤±è´¥ '{primitive_name}': {e}")
                    raise

            return result

        composed_tool.__name__ = name
        composed_tool.__doc__ = spec.get("description", "Composed tool")

        return composed_tool

    def _resolve_templates(self, args: Dict, result, context: Dict) -> Dict:
        """è§£ææ¨¡æ¿å˜é‡"""
        resolved = {}
        for key, value in args.items():
            if isinstance(value, str) and "{" in value:
                # ç®€å•æ¨¡æ¿æ›¿æ¢
                resolved[key] = value.format(**context)
            else:
                resolved[key] = value
        return resolved

    def register_from_spec(self, spec: Dict):
        """å°†ç»„åˆå·¥å…·æ³¨å†Œåˆ°å·¥å…·æ³¨å†Œè¡¨"""
        from core.tool_registry import registry, Tool

        composed_func = self.compose_tool(spec)
        tool = Tool(composed_func, name=spec["name"], description=spec["description"])
        registry.register(tool)

        return tool
```

**é›†æˆç‚¹**:
- åœ¨å¯åŠ¨æ—¶åŠ è½½é¢„å®šä¹‰çš„å·¥å…·é…ç½®ï¼ˆYAML æ–‡ä»¶ï¼‰
- å…è®¸ LLM å»ºè®®æ–°çš„å·¥å…·ç»„åˆï¼ˆä½†éœ€äººå·¥å®¡æ‰¹ï¼‰

**ç¤ºä¾‹å·¥å…·é…ç½®**: `configs/composed_tools.yaml`
```yaml
tools:
  - name: "get_weather"
    description: "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”"
    steps:
      - primitive: "http_get"
        args:
          url: "https://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}"
      - primitive: "parse_json"
      - primitive: "extract_field"
        args:
          field: "main.temp"
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ”¯æŒ 20+ å®‰å…¨åŸè¯­
- âœ… ä» YAML åŠ è½½å·¥å…·é…ç½®
- âœ… ç¦æ­¢ä»»æ„ä»£ç æ‰§è¡Œ
- âœ… å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—

---

### Phase 2: å¤š Agent åä½œ (Multi-Agent) - 2 å‘¨

#### Task 2.1: Sub-Agent Orchestrator (7 å¤©)
**æ–°å»ºæ–‡ä»¶**: `core/orchestrator.py`

```python
import asyncio
from typing import List, Dict, Any, Optional
import networkx as nx

class SubTask(BaseModel):
    """å­ä»»åŠ¡å®šä¹‰"""
    id: str
    description: str
    assigned_agent: str
    dependencies: List[str] = []
    timeout_seconds: int = 300
    max_retries: int = 2

class TaskResult(BaseModel):
    """ä»»åŠ¡ç»“æœ"""
    task_id: str
    success: bool
    result: Any
    duration: float
    error: Optional[str] = None

class AgentOrchestrator:
    """å¤š Agent ç¼–æ’å™¨"""

    def __init__(self, max_concurrent_agents: int = 3):
        self.agents = {}  # agent_name -> agent_instance
        self.max_concurrent = max_concurrent_agents
        self.task_graph = nx.DiGraph()

    def register_agent(self, name: str, agent: Any):
        """æ³¨å†Œ Agent"""
        self.agents[name] = agent

    async def execute_swarm(self, subtasks: List[SubTask]) -> Dict[str, TaskResult]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå­ä»»åŠ¡"""
        # 1. æ„å»ºä¾èµ–å›¾
        self._build_dependency_graph(subtasks)

        # 2. æ£€æµ‹å¾ªç¯ä¾èµ–
        if not self._is_dag():
            raise ValueError("æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–ï¼Œæ— æ³•æ‰§è¡Œ")

        # 3. æ‹“æ‰‘æ’åº
        execution_order = list(nx.topological_sort(self.task_graph))

        # 4. æŒ‰å±‚çº§å¹¶è¡Œæ‰§è¡Œ
        results = {}
        for layer in self._get_execution_layers(execution_order, subtasks):
            layer_results = await self._execute_layer(layer, results)
            results.update(layer_results)

        return results

    def _build_dependency_graph(self, subtasks: List[SubTask]):
        """æ„å»ºä»»åŠ¡ä¾èµ–å›¾"""
        self.task_graph.clear()

        for task in subtasks:
            self.task_graph.add_node(task.id, task=task)
            for dep in task.dependencies:
                self.task_graph.add_edge(dep, task.id)

    def _is_dag(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰å‘æ— ç¯å›¾"""
        try:
            list(nx.topological_sort(self.task_graph))
            return True
        except nx.NetworkXError:
            return False

    def _get_execution_layers(self, order: List[str], subtasks: List[SubTask]) -> List[List[SubTask]]:
        """å°†ä»»åŠ¡åˆ†å±‚ï¼ˆåŒå±‚å¯å¹¶è¡Œæ‰§è¡Œï¼‰"""
        task_map = {t.id: t for t in subtasks}
        layers = []
        completed = set()

        while len(completed) < len(subtasks):
            # æ‰¾å‡ºæ‰€æœ‰ä¾èµ–å·²å®Œæˆçš„ä»»åŠ¡
            current_layer = []
            for task_id in order:
                if task_id in completed:
                    continue

                task = task_map[task_id]
                if all(dep in completed for dep in task.dependencies):
                    current_layer.append(task)

            if not current_layer:
                raise RuntimeError("æ— æ³•æ‰¾åˆ°å¯æ‰§è¡Œçš„ä»»åŠ¡å±‚")

            layers.append(current_layer)
            completed.update(t.id for t in current_layer)

        return layers

    async def _execute_layer(self, layer: List[SubTask], previous_results: Dict) -> Dict[str, TaskResult]:
        """å¹¶è¡Œæ‰§è¡Œä¸€å±‚ä»»åŠ¡"""
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def execute_with_semaphore(task: SubTask):
            async with semaphore:
                return await self._execute_single_task(task, previous_results)

        results = await asyncio.gather(
            *[execute_with_semaphore(task) for task in layer],
            return_exceptions=True
        )

        return {
            task.id: result if isinstance(result, TaskResult) else TaskResult(
                task_id=task.id,
                success=False,
                result=None,
                duration=0,
                error=str(result)
            )
            for task, result in zip(layer, results)
        }

    async def _execute_single_task(self, task: SubTask, context: Dict) -> TaskResult:
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        agent = self.agents.get(task.assigned_agent)
        if not agent:
            raise ValueError(f"Agent ä¸å­˜åœ¨: {task.assigned_agent}")

        start_time = time.time()

        try:
            # æ‰§è¡Œä»»åŠ¡ï¼ˆæ ¹æ® agent ç±»å‹è°ƒç”¨ä¸åŒæ–¹æ³•ï¼‰
            if hasattr(agent, 'execute_task'):
                result = await asyncio.wait_for(
                    agent.execute_task(task.description),
                    timeout=task.timeout_seconds
                )
            else:
                raise NotImplementedError(f"Agent {task.assigned_agent} æ²¡æœ‰ execute_task æ–¹æ³•")

            duration = time.time() - start_time

            return TaskResult(
                task_id=task.id,
                success=True,
                result=result,
                duration=duration
            )

        except asyncio.TimeoutError:
            return TaskResult(
                task_id=task.id,
                success=False,
                result=None,
                duration=task.timeout_seconds,
                error="ä»»åŠ¡è¶…æ—¶"
            )
        except Exception as e:
            return TaskResult(
                task_id=task.id,
                success=False,
                result=None,
                duration=time.time() - start_time,
                error=str(e)
            )
```

**é›†æˆç‚¹**:
- åœ¨ `main_v3.py` ä¸­åˆ›å»º Orchestrator
- æ³¨å†Œ Planner, Executor, Researcher ä¸ºå¯è°ƒåº¦çš„ Agent
- å½“ä»»åŠ¡å¤æ‚æ—¶ï¼Œä½¿ç”¨ LLM åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ”¯æŒä»»åŠ¡ä¾èµ–ç®¡ç†
- âœ… è‡ªåŠ¨æ£€æµ‹å¾ªç¯ä¾èµ–
- âœ… åŒå±‚ä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ
- âœ… å•ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡

---

### Phase 3: å®‰å…¨ä¸ç¨³å®šæ€§ (Safety & Stability) - 1 å‘¨

#### Task 3.1: æ²™ç®±æ‰§è¡Œç³»ç»Ÿ (å¯é€‰ï¼Œä»…ç”¨äºé«˜é£é™©åœºæ™¯) (5 å¤©)

**è­¦å‘Š**: è¿™æ˜¯é«˜é£é™©åŠŸèƒ½ï¼Œå»ºè®®ä»…åœ¨ç¡®å®éœ€è¦æ—¶å®ç°

**å®ç°æ–¹æ¡ˆ**: ä½¿ç”¨ Docker å®¹å™¨éš”ç¦»

**æ–°å»ºæ–‡ä»¶**: `core/sandbox.py`

```python
import docker
from pathlib import Path

class DockerSandbox:
    """Docker æ²™ç®±æ‰§è¡Œå™¨"""

    def __init__(self, image: str = "python:3.11-slim"):
        self.client = docker.from_env()
        self.image = image

    def run_code(
        self,
        code: str,
        timeout: int = 30,
        memory_limit: str = "256m",
        network_disabled: bool = True
    ) -> Dict:
        """åœ¨éš”ç¦»å®¹å™¨ä¸­æ‰§è¡Œä»£ç """

        # 1. åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
        work_dir = Path("/tmp/sandbox") / str(uuid.uuid4())
        work_dir.mkdir(parents=True, exist_ok=True)

        # 2. å†™å…¥ä»£ç 
        code_file = work_dir / "main.py"
        code_file.write_text(code)

        try:
            # 3. è¿è¡Œå®¹å™¨
            container = self.client.containers.run(
                self.image,
                command=f"python /sandbox/main.py",
                volumes={str(work_dir): {"bind": "/sandbox", "mode": "ro"}},
                mem_limit=memory_limit,
                network_disabled=network_disabled,
                detach=True,
                remove=True
            )

            # 4. ç­‰å¾…ç»“æœ
            result = container.wait(timeout=timeout)
            logs = container.logs().decode()

            return {
                "success": result["StatusCode"] == 0,
                "output": logs,
                "exit_code": result["StatusCode"]
            }

        except docker.errors.ContainerError as e:
            return {
                "success": False,
                "output": str(e),
                "exit_code": -1,
                "error": "å®¹å™¨æ‰§è¡Œé”™è¯¯"
            }
        except Exception as e:
            return {
                "success": False,
                "output": "",
                "exit_code": -1,
                "error": str(e)
            }
        finally:
            # æ¸…ç†
            shutil.rmtree(work_dir, ignore_errors=True)
```

**ä½¿ç”¨åœºæ™¯**:
- âš ï¸ ä»…åœ¨éœ€è¦æ‰§è¡Œä¸å¯ä¿¡ä»£ç æ—¶ä½¿ç”¨
- âš ï¸ å»ºè®®ä¼˜å…ˆä½¿ç”¨ Tool Composer

**éªŒæ”¶æ ‡å‡†**:
- âœ… ä»£ç åœ¨éš”ç¦»ç¯å¢ƒæ‰§è¡Œ
- âœ… è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶
- âœ… å†…å­˜/CPU é™åˆ¶ç”Ÿæ•ˆ

---

## ğŸ“Š ä¼˜å…ˆçº§çŸ©é˜µ

| Phase | ä»»åŠ¡ | å·¥ä½œé‡ | ä»·å€¼ | é£é™© | ä¼˜å…ˆçº§ |
|-------|-----|--------|-----|------|-------|
| **Phase 0** | é¢„ç®—ç®¡ç† | 2å¤© | â­â­â­â­â­ | ğŸŸ¢ | ğŸ”´ P0 |
| **Phase 0** | OpenTelemetry | 3å¤© | â­â­â­â­ | ğŸŸ¢ | ğŸŸ  P1 |
| **Phase 0** | Checkpoint/Rollback | 2å¤© | â­â­â­â­ | ğŸŸ¡ | ğŸŸ  P1 |
| **Phase 1** | Persona ä¼˜åŒ– | 3å¤© | â­â­â­ | ğŸŸ¡ | ğŸŸ  P1 |
| **Phase 1** | Researcher NLI | 4å¤© | â­â­â­â­ | ğŸŸ¡ | ğŸŸ  P1 |
| **Phase 1** | Tool Composer | 5å¤© | â­â­â­â­â­ | ğŸŸ  | ğŸŸ  P1 |
| **Phase 2** | Sub-Agent Orchestrator | 7å¤© | â­â­â­â­â­ | ğŸŸ  | ğŸŸ¡ P2 |
| **Phase 3** | Docker æ²™ç®± | 5å¤© | â­â­ | ğŸ”´ | ğŸŸ¢ P3 (å¯é€‰) |

---

## ğŸ› ï¸ æŠ€æœ¯æ ˆæ¨è

### æ ¸å¿ƒä¾èµ–

```bash
# ç°æœ‰ä¾èµ–
pydantic>=2.0
pyyaml
anthropic

# Phase 0 æ–°å¢
opentelemetry-api>=1.20.0
opentelemetry-sdk>=1.20.0
opentelemetry-exporter-jaeger>=1.20.0  # å¯é€‰

# Phase 1 æ–°å¢
sentence-transformers>=2.2.0  # Reranking
transformers>=4.35.0          # NLI
torch>=2.0.0                  # æ¨¡å‹æ¨ç†

# Phase 2 æ–°å¢
networkx>=3.0                 # ä¾èµ–å›¾
aiohttp>=3.9.0                # å¼‚æ­¥ HTTP

# Phase 3 æ–°å¢ (å¯é€‰)
docker>=6.0.0                 # æ²™ç®±
```

### å¯é€‰æœåŠ¡

- **Jaeger** (åˆ†å¸ƒå¼è¿½è¸ªå¯è§†åŒ–): `docker run -d -p 6831:6831/udp -p 16686:16686 jaegertracing/all-in-one:latest`
- **Cohere API** (é‡æ’åºæœåŠ¡): æ›¿ä»£æœ¬åœ°æ¨¡å‹ï¼Œæ›´å¿«ä½†ä»˜è´¹

---

## ğŸ“ˆ æ¸è¿›å¼éƒ¨ç½²ç­–ç•¥

### ç¬¬ 1-2 å‘¨ï¼šåŸºç¡€è®¾æ–½ (Phase 0)
- âœ… é¢„ç®—ç®¡ç† (é˜²æ­¢æˆæœ¬å¤±æ§)
- âœ… OpenTelemetry (æå‡è°ƒè¯•èƒ½åŠ›)
- âœ… Checkpoint (é”™è¯¯æ¢å¤)

**äº§å‡º**: V3.1-alpha (å†…éƒ¨æµ‹è¯•ç‰ˆ)

### ç¬¬ 3-4 å‘¨ï¼šæ ¸å¿ƒå¢å¼º (Phase 1)
- âœ… Persona ä¼˜åŒ– (é™ä½æˆæœ¬)
- âœ… Researcher å¢å¼º (æå‡è´¨é‡)
- âœ… Tool Composer (æ‰©å±•èƒ½åŠ›)

**äº§å‡º**: V3.1-beta (åŠŸèƒ½å®Œæ•´ç‰ˆ)

### ç¬¬ 5-6 å‘¨ï¼šå¤š Agent (Phase 2)
- âœ… Sub-Agent Orchestrator (å¤æ‚ä»»åŠ¡åˆ†è§£)

**äº§å‡º**: V3.1-rc (å€™é€‰å‘å¸ƒç‰ˆ)

### ç¬¬ 7 å‘¨ï¼šæµ‹è¯•ä¸ä¼˜åŒ–
- ğŸ§ª é›†æˆæµ‹è¯•
- ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•
- ğŸ“ æ–‡æ¡£å®Œå–„

**äº§å‡º**: V3.1 (ç”Ÿäº§ç‰ˆæœ¬)

---

## ğŸ¯ æˆåŠŸæŒ‡æ ‡ (KPIs)

| æŒ‡æ ‡ | å½“å‰ V3.0 | ç›®æ ‡ V3.1 | æµ‹é‡æ–¹æ³• |
|-----|----------|----------|---------|
| **å¹³å‡æˆæœ¬/ä»»åŠ¡** | æœªçŸ¥ | < $0.50 | Cost Tracker |
| **ä»»åŠ¡æˆåŠŸç‡** | ~60% | > 85% | Success Rate |
| **å¹³å‡å“åº”æ—¶é—´** | ~120s | < 90s | OpenTelemetry |
| **ç ”ç©¶è´¨é‡** | æœªè¯„ä¼° | > 0.75 | NLI Score |
| **é”™è¯¯æ¢å¤ç‡** | 0% | > 90% | Checkpoint ä½¿ç”¨ç‡ |

---

## ğŸš¨ é£é™©ä¸ç¼“è§£

| é£é™© | ä¸¥é‡æ€§ | ç¼“è§£æªæ–½ |
|-----|--------|---------|
| **é¢„ç®—è¶…æ”¯** | ğŸ”´ æé«˜ | Phase 0 ä¼˜å…ˆå®ç°é¢„ç®—ç®¡ç† |
| **ä¾èµ–å®‰è£…å¤±è´¥** | ğŸŸ  é«˜ | æä¾› Docker é•œåƒ |
| **NLI æ¨¡å‹å†…å­˜å ç”¨** | ğŸŸ¡ ä¸­ | ä½¿ç”¨ ONNX é‡åŒ– / Cohere API |
| **æ²™ç®±é€ƒé€¸** | ğŸ”´ æé«˜ | Phase 3 å¯é€‰ï¼Œéœ€ä¸¥æ ¼å®¡è®¡ |
| **å¤š Agent æ­»é”** | ğŸŸ  é«˜ | ä¾èµ–å›¾æ£€æµ‹ + è¶…æ—¶æœºåˆ¶ |

---

## ğŸ“š å‚è€ƒèµ„æº

### æŠ€æœ¯æ–‡æ¡£
- [OpenTelemetry Python SDK](https://opentelemetry.io/docs/instrumentation/python/)
- [Sentence Transformers (Reranking)](https://www.sbert.net/examples/applications/cross-encoder/README.html)
- [NetworkX (å›¾è®º)](https://networkx.org/documentation/stable/)

### è®ºæ–‡
- DSPy: Compiling Declarative Language Model Calls (Stanford, 2023)
- Self-Refine: Iterative Refinement with Self-Feedback (2024)

### ç«å“åˆ†æ
- **AutoGen** (Microsoft): å¤š Agent åä½œå‚è€ƒ
- **LangGraph** (LangChain): çŠ¶æ€æœºå¼ç¼–æ’å‚è€ƒ
- **CrewAI**: Persona åˆ‡æ¢å‚è€ƒ

---

## âœ… éªŒæ”¶æ¸…å•

### Phase 0 å®Œæˆæ ‡å‡†
- [ ] é¢„ç®—ç®¡ç†ç³»ç»Ÿå¯è¿è¡Œï¼Œæµ‹è¯•è¦†ç›–ç‡ > 80%
- [ ] OpenTelemetry é›†æˆï¼ŒJaeger UI å¯æŸ¥çœ‹é“¾è·¯
- [ ] Checkpoint æœºåˆ¶éªŒè¯ï¼Œå¯æ‰‹åŠ¨å›æ»š

### Phase 1 å®Œæˆæ ‡å‡†
- [ ] Persona ä¼˜åŒ–å token å‡å°‘ > 20%
- [ ] Researcher NLI éªŒè¯å‡†ç¡®ç‡ > 70%
- [ ] Tool Composer æ”¯æŒ 20+ åŸè¯­

### Phase 2 å®Œæˆæ ‡å‡†
- [ ] Orchestrator æ”¯æŒä»»åŠ¡ä¾èµ–ç®¡ç†
- [ ] å¹¶è¡Œæ‰§è¡Œ 3+ ä»»åŠ¡æ— æ­»é”

### Phase 3 å®Œæˆæ ‡å‡†
- [ ] æ²™ç®±éš”ç¦»éªŒè¯ï¼ˆå¦‚å®ç°ï¼‰

---

## ğŸ“ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³å¼€å§‹**: Phase 0 Task 0.1 é¢„ç®—ç®¡ç†ç³»ç»Ÿ
2. **å¹¶è¡Œç ”ç©¶**: è¯„ä¼° Cohere Rerank vs. æœ¬åœ°æ¨¡å‹
3. **åˆ›å»ºåˆ†æ”¯**: `feature/v3.1-production`
4. **è®¾ç½®é‡Œç¨‹ç¢‘**: GitHub Issues è·Ÿè¸ªæ¯ä¸ª Task

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-11-21
**ä½œè€…**: Claude + Human Collaboration
**çŠ¶æ€**: å¾…æ‰§è¡Œ

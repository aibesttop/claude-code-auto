"""
Mirror Analysis Module - v1.0 Core Feature

Creates isolated mirror environments for AI to analyze task progress
without polluting the actual working directory.

This is the "soul" of v1.0 autonomous system.
"""
import os
import shutil
import json
from pathlib import Path
from typing import Dict, Tuple, Optional
import logging

from src.core.agents.sdk_client import run_claude_prompt

logger = logging.getLogger(__name__)


class MirrorAnalyzer:
    """
    é•œåƒåˆ†æå™¨ - v1.0æ ¸å¿ƒæœºåˆ¶

    åœ¨éš”ç¦»çš„é•œåƒç¯å¢ƒä¸­è®©AIåˆ†æä»»åŠ¡è¿›åº¦ï¼Œ
    è¿”å›AIçš„è‡ªä¸»åˆ¤æ–­è€Œéè§„åˆ™éªŒè¯ã€‚
    """

    def __init__(self, work_dir: str, mirror_base: str = None):
        """
        Initialize mirror analyzer.

        Args:
            work_dir: Working directory to analyze
            mirror_base: Base directory for mirrors (default: work_dir/../mirror)
        """
        self.work_dir = Path(work_dir)

        if mirror_base:
            self.mirror_base = Path(mirror_base)
        else:
            self.mirror_base = self.work_dir.parent / "mirror"

        self.mirror_base.mkdir(parents=True, exist_ok=True)

    def create_mirror(self, mirror_name: str = None) -> Path:
        """
        Create a mirror copy of the working directory.

        Args:
            mirror_name: Name for the mirror (default: work_dir_name + _mirror)

        Returns:
            Path to the created mirror directory
        """
        if not mirror_name:
            mirror_name = f"{self.work_dir.name}_mirror"

        mirror_path = self.mirror_base / mirror_name

        # Remove existing mirror (with Windows compatibility)
        if mirror_path.exists():
            logger.debug(f"Removing existing mirror: {mirror_path}")
            self._remove_mirror_safe(mirror_path)

        # Copy work_dir to mirror
        logger.info(f"Creating mirror: {self.work_dir} -> {mirror_path}")
        shutil.copytree(self.work_dir, mirror_path)

        # Clean up sensitive files in mirror
        self._cleanup_mirror(mirror_path)

        logger.info(f"âœ… Mirror created: {mirror_path}")
        return mirror_path

    def _remove_mirror_safe(self, path: Path, max_retries: int = 3):
        """
        Safely remove mirror directory with Windows file locking handling.

        Args:
            path: Path to remove
            max_retries: Maximum number of retry attempts
        """
        import time
        import stat

        def handle_remove_readonly(func, path, exc):
            """Error handler for Windows readonly files."""
            os.chmod(path, stat.S_IWRITE)
            func(path)

        for attempt in range(max_retries):
            try:
                # Try to remove with readonly handler for Windows
                shutil.rmtree(path, onerror=handle_remove_readonly)
                return
            except PermissionError as e:
                if attempt < max_retries - 1:
                    logger.warning(f"Mirror removal failed (attempt {attempt + 1}/{max_retries}): {e}")
                    time.sleep(0.5 * (attempt + 1))  # Exponential backoff
                else:
                    # Last resort: rename to .old instead of deleting
                    old_path = path.parent / f"{path.name}.old.{int(time.time())}"
                    logger.warning(f"Could not remove mirror, renaming to: {old_path}")
                    try:
                        path.rename(old_path)
                    except Exception as rename_error:
                        logger.error(f"Failed to rename mirror: {rename_error}")
                        raise
            except Exception as e:
                logger.error(f"Unexpected error removing mirror: {e}")
                raise

    def _cleanup_mirror(self, mirror_path: Path):
        """Remove sensitive files from mirror."""
        # Remove session files
        session_files = [
            "session_id.txt",
            ".session",
            "execution_state.json"
        ]

        for file_name in session_files:
            file_path = mirror_path / file_name
            if file_path.exists():
                file_path.unlink()
                logger.debug(f"Cleaned from mirror: {file_name}")

    async def ai_analyze_progress(
        self,
        goal: str,
        role_name: str = None,
        context: str = None,
        model: str = None,
        timeout: int = 120
    ) -> Tuple[bool, str, str]:
        """
        è®©AIåœ¨é•œåƒç¯å¢ƒä¸­åˆ†æä»»åŠ¡è¿›åº¦ï¼ˆv1.0æ ¸å¿ƒæœºåˆ¶ï¼‰

        Args:
            goal: Task goal to evaluate
            role_name: Role name (for context)
            context: Additional context for AI
            model: Claude model to use
            timeout: Timeout in seconds

        Returns:
            Tuple of (completed, next_action, analysis)
            - completed: bool - AIåˆ¤æ–­ä»»åŠ¡æ˜¯å¦å®Œæˆå¾—å¥½
            - next_action: str - AIå»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨
            - analysis: str - AIçš„åˆ†æè¯´æ˜
        """
        # Create mirror for analysis
        mirror_name = f"{role_name}_mirror" if role_name else "analysis_mirror"

        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ” AI AUTONOMOUS ANALYSIS - Mirror Environment")
        logger.info(f"{'='*70}")
        logger.info(f"ğŸ“ Creating mirror for isolated analysis...")

        mirror_path = self.create_mirror(mirror_name)
        logger.info(f"âœ… Mirror created: {mirror_path}")
        logger.info(f"ğŸ¯ Analyzing goal: {goal[:100]}{'...' if len(goal) > 100 else ''}")
        if role_name:
            logger.info(f"ğŸ‘¤ Role: {role_name}")

        # Construct AI analysis prompt
        prompt = self._build_analysis_prompt(goal, role_name, context)
        logger.info(f"ğŸ“ AI analysis prompt prepared ({len(prompt)} chars)")

        try:
            logger.info(f"ğŸ¤– Invoking Claude AI for analysis...")
            # Run Claude in mirror environment
            response_text, _ = await run_claude_prompt(
                prompt=prompt,
                work_dir=str(mirror_path),  # â† Fixed: use work_dir instead of cwd
                model=model,
                permission_mode="bypassPermissions",
                timeout=timeout
            )

            logger.info(f"âœ… AI analysis completed, parsing response...")

            # Parse AI's JSON response
            completed, next_action, analysis, result_dict = self._parse_ai_response(response_text)

            # Extract additional details
            quality_score = result_dict.get("quality_score", 0)
            improvement_suggestions = result_dict.get("improvement_suggestions", [])

            # Display detailed analysis results
            logger.info(f"\n{'â”€'*70}")
            logger.info(f"ğŸ“Š AI JUDGMENT RESULTS")
            logger.info(f"{'â”€'*70}")
            logger.info(f"ğŸ¯ Quality Score: {quality_score}/10")

            if quality_score >= 8:
                logger.info(f"âœ… Status: EXCELLENT - Task can be completed")
            elif quality_score >= 6:
                logger.info(f"âš ï¸  Status: GOOD - Minor improvements suggested")
            elif quality_score >= 4:
                logger.info(f"âŒ Status: AVERAGE - Significant improvements needed")
            else:
                logger.info(f"ğŸ”´ Status: POOR - Major rework required")

            logger.info(f"")
            logger.info(f"AI Decision: {'âœ… COMPLETED' if completed else 'â³ CONTINUE IMPROVING'}")
            logger.info(f"")
            logger.info(f"ğŸ“ AI Analysis:")
            # Format analysis text with indentation
            for line in analysis.split('\n'):
                if line.strip():
                    logger.info(f"   {line.strip()}")

            if not completed:
                logger.info(f"")
                logger.info(f"ğŸ’¡ Next Action Suggested:")
                logger.info(f"   {next_action}")

            if improvement_suggestions:
                logger.info(f"")
                logger.info(f"ğŸ”§ Improvement Suggestions:")
                for i, suggestion in enumerate(improvement_suggestions, 1):
                    logger.info(f"   {i}. {suggestion}")

            logger.info(f"{'='*70}\n")

            return completed, next_action, analysis

        except Exception as e:
            logger.error(f"\n{'!'*70}")
            logger.error(f"âŒ AI ANALYSIS FAILED")
            logger.error(f"{'!'*70}")
            logger.error(f"Error: {e}")
            logger.error(f"{'!'*70}\n")
            # Fallback: assume not completed
            return False, "AIåˆ†æå‡ºé”™ï¼Œè¯·æ£€æŸ¥å·¥ä½œç›®å½•å¹¶ç»§ç»­", str(e)

    def _build_analysis_prompt(
        self,
        goal: str,
        role_name: str = None,
        context: str = None
    ) -> str:
        """
        æ„å»ºAIåˆ†ææç¤ºè¯ï¼ˆv1.0é£æ ¼ï¼‰
        """
        role_context = f"ä½œä¸º **{role_name}** è§’è‰²ï¼Œ" if role_name else ""
        additional_context = f"\n\nè¡¥å……ä¿¡æ¯ï¼š\n{context}" if context else ""

        prompt = f"""
è¯·åˆ†æå½“å‰å·¥ä½œç›®å½•ä¸­çš„æ–‡ä»¶å’Œå†…å®¹ï¼Œåˆ¤æ–­ä»¥ä¸‹ä»»åŠ¡çš„å®Œæˆæƒ…å†µï¼š

**ä»»åŠ¡ç›®æ ‡**ï¼š{goal}

{role_context}ä½ éœ€è¦è¯„ä¼°ï¼š
1. å½“å‰ä»»åŠ¡æ˜¯å¦å·²ç»å®Œæˆå¾—è¶³å¤Ÿå¥½ï¼Ÿ
2. å¦‚æœæœªå®Œæˆæˆ–å¯ä»¥æ”¹è¿›ï¼Œä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆï¼Ÿ
3. å¯¹å½“å‰å·¥ä½œæˆæœçš„è´¨é‡è¯„ä»·

{additional_context}

**é‡è¦**ï¼šè¯·ä»¥JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{{
    "completed": true/false,
    "next_action": "å¦‚æœæœªå®Œæˆï¼Œæè¿°ä¸‹ä¸€æ­¥å…·ä½“è¡ŒåŠ¨ï¼›å¦‚æœå·²å®Œæˆï¼Œç•™ç©º",
    "analysis": "å¯¹å½“å‰çŠ¶æ€çš„è¯¦ç»†åˆ†æå’Œè¯„ä»·",
    "quality_score": 0-10,
    "improvement_suggestions": ["å»ºè®®1", "å»ºè®®2"]
}}
```

è¯„åˆ†æ ‡å‡†ï¼ˆquality_scoreï¼‰ï¼š
- 8-10åˆ†ï¼šä¼˜ç§€ï¼Œå¯ä»¥å®Œæˆ
- 6-7åˆ†ï¼šè‰¯å¥½ï¼Œå»ºè®®å°å¹…æ”¹è¿›
- 4-5åˆ†ï¼šä¸€èˆ¬ï¼Œéœ€è¦æ˜æ˜¾æ”¹è¿›
- 0-3åˆ†ï¼šä¸åˆæ ¼ï¼Œå¿…é¡»é‡åš

**åªæœ‰quality_score >= 8æ—¶ï¼Œæ‰è®¾ç½®completed=true**

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼å›å¤ï¼Œä¸è¦æ·»åŠ é¢å¤–çš„è¯´æ˜æ–‡å­—ã€‚
"""
        return prompt

    def _parse_ai_response(self, response_text: str) -> Tuple[bool, str, str, Dict]:
        """
        è§£æAIçš„JSONå“åº”

        Returns:
            (completed, next_action, analysis, result_dict)
        """
        try:
            # Extract JSON from response
            start_idx = response_text.find("{")
            end_idx = response_text.rfind("}") + 1

            if start_idx == -1 or end_idx == 0:
                logger.warning("AIå“åº”ä¸­æœªæ‰¾åˆ°JSONæ ¼å¼")
                return False, "è¯·æ£€æŸ¥å¹¶æ”¹è¿›å·¥ä½œ", "å“åº”æ ¼å¼é”™è¯¯", {}

            json_str = response_text[start_idx:end_idx]
            result = json.loads(json_str)

            # Extract fields
            completed = result.get("completed", False)
            next_action = result.get("next_action", "")
            analysis = result.get("analysis", "")
            quality_score = result.get("quality_score", 0)

            # Ensure completed only if quality is high enough
            if completed and quality_score < 8:
                logger.debug(f"âš ï¸ Quality score {quality_score} < 8, overriding completed to False")
                completed = False
                if not next_action:
                    next_action = "æå‡è´¨é‡è‡³8åˆ†ä»¥ä¸Š"

            return completed, next_action, analysis, result

        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            logger.debug(f"åŸå§‹å“åº”: {response_text[:500]}...")
            return False, "JSONè§£æé”™è¯¯ï¼Œè¯·ç»§ç»­å·¥ä½œ", str(e), {}
        except Exception as e:
            logger.error(f"å“åº”è§£æå¼‚å¸¸: {e}")
            return False, str(e), "è§£æå¼‚å¸¸", {}

    def cleanup_mirrors(self):
        """æ¸…ç†æ‰€æœ‰é•œåƒç›®å½•"""
        if self.mirror_base.exists():
            shutil.rmtree(self.mirror_base)
            logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç†é•œåƒç›®å½•: {self.mirror_base}")


# Convenience function
async def ai_judge_task_completion(
    work_dir: str,
    goal: str,
    role_name: str = None,
    context: str = None
) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨AIåˆ¤æ–­ä»»åŠ¡å®Œæˆæƒ…å†µï¼ˆv1.0æœºåˆ¶ï¼‰

    Returns:
        {
            "completed": bool,
            "next_action": str,
            "analysis": str,
            "should_continue": bool  # True if should continue improving
        }
    """
    analyzer = MirrorAnalyzer(work_dir)

    completed, next_action, analysis, _ = await analyzer.ai_analyze_progress(
        goal=goal,
        role_name=role_name,
        context=context
    )

    return {
        "completed": completed,
        "next_action": next_action,
        "analysis": analysis,
        "should_continue": not completed
    }
